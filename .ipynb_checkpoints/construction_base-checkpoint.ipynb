{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Préparation des données\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modèles\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "import time\n",
    "\n",
    "# Option d'affchage\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilian/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning:\n",
      "\n",
      "Columns (13,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0 = pd.read_csv('/Users/lilian/Desktop/hackathon2021/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['store_id'].isin(list(df['store_id'].sample(10)))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction de variables intéressantes à partir des préexistantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note moyenne\n",
    "\n",
    "def compute_note(overall, count):\n",
    "    try:\n",
    "        overall / count\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df['note_moyenne'] = df.apply(lambda row : compute_note(row.sum_rating_overall, row.rating_count) , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lifetime\n",
    "\n",
    "col_date = ['date', 'items_first_enabled_date', 'store_last_saving_date', 'store_first_saving_date',\\\n",
    "            'pickup_start', 'pickup_end']\n",
    "for col in col_date :\n",
    "    df[col]= pd.to_datetime(df[col])\n",
    "\n",
    "df['lifetime'] = df['store_last_saving_date'] - df['store_first_saving_date']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction\n",
    "\n",
    "df['reduction'] = 1 - df['item_price'] / df['before_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temps d'ouverture\n",
    "\n",
    "df['temps_ouverture'] = df['pickup_end'] - df['pickup_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heure de début d'ouverture\n",
    "\n",
    "df['heure_debut_ouverture'] = df.apply(lambda row : row.pickup_start.hour, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficacité\n",
    "\n",
    "df['efficacite'] = df['meals_saved'] / df['total_supply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Franchise\n",
    "\n",
    "df['franchise'] = df.apply(lambda row : int(row.parent_chain_id > 0), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de nouvelles variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recodage de la variable objectif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_absence_future(date, store_id):\n",
    "    df_short = df[df['store_id'] == store_id]\n",
    "    df_short = df_short[df_short['date'] > date]\n",
    "    \n",
    "    serie = list(df_short['total_supply'])\n",
    "    \n",
    "    i = 0\n",
    "    l = len(serie)\n",
    "    try:\n",
    "        while serie[i] == 0 and i < l:\n",
    "            i+=1\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    return i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['absence_future'] = df.apply(lambda row : determine_absence_future(row.date, row.store_id), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance du nombre de vente pour un shop\n",
    "\n",
    "def compute_variance(date, store_id, variable):\n",
    "    \n",
    "    df_short = df[df['store_id'] == store_id]\n",
    "    df_short = df_short[df_short['date'] < date]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_short[variable].var()\n",
    "\n",
    "df['variance_ventes'] = df.apply(lambda row : compute_variance(row.date, row.store_id, 'meals_saved'), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baisse du nombre de vente dans le mois précedent\n",
    "\n",
    "def compute_baisse_vente(date, store_id, variable):\n",
    "    df_short = df[df['store_id'] == store_id]\n",
    "    df_short = df_short[df_short['date'] < date]\n",
    "    serie = df_short[variable]\n",
    "    \n",
    "    try:\n",
    "        b = int(np.mean(serie[-30:]) / np.mean(serie[:-30]) < 1)\n",
    "        \n",
    "    except:\n",
    "        b = 0\n",
    "        \n",
    "    return b\n",
    "\n",
    "df['baisse_ventes'] = df.apply(lambda row : compute_baisse_vente(row.date, row.store_id, 'meals_saved'), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation de la réduction durant la semaine précedente\n",
    "\n",
    "\n",
    "def compute_hausse_reduction(date, store_id, variable):\n",
    "    df_short = df[df['store_id'] == store_id]\n",
    "    df_short = df_short[df_short['date'] < date]\n",
    "    serie = df_short[variable]\n",
    "    \n",
    "    try:\n",
    "        b = int(np.mean(serie[-7:]) / np.mean(serie[:-7]) > 1)\n",
    "        if b:\n",
    "            print(ok)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        b = 0\n",
    "        \n",
    "    return b\n",
    "\n",
    "df['hausse_reduction'] = df.apply(lambda row : compute_hausse_reduction(row.date, row.store_id, 'reduction'), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Variance de la durée d'ouverture\n",
    "\n",
    "def compute_variance(date, store_id, variable):\n",
    "    \n",
    "    df_short = df[df['store_id'] == store_id]\n",
    "    #print(df_short.shape)\n",
    "    df_short = df_short[df_short['date'] < date]\n",
    "    display(df_short)\n",
    "    serie = df_short[variable]\n",
    "    #print(serie)\n",
    "    variance = np.var([(i.seconds / 3600) for i in serie])\n",
    "    \n",
    "    return variance\n",
    "\n",
    "#df['variance_duree_ouverture'] = df.apply(lambda row : compute_variance(row.date, row.store_id, 'temps_ouverture'), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baisse de la note moyenne durant la semaine précedente\n",
    "\n",
    "def compute_baisse_note(date, store_id, variable):\n",
    "    df_short = df[df['store_id'] == store_id]\n",
    "    df_short = df_short[df_short['date'] < date]\n",
    "    serie = df_short[variable]\n",
    "    \n",
    "    try:\n",
    "        b = np.mean(serie[-7:]) / np.mean(serie[:-7]) < 1\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        b = 0\n",
    "        \n",
    "    return b\n",
    "\n",
    "df['baisse_note'] = df.apply(lambda row : compute_baisse_note(row.date, row.store_id, 'note_moyenne'), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation du nombre d'invendus durant le mois précédent\n",
    "\n",
    "def compute_hausse_invendus(date, store_id, variable):\n",
    "    df_short = df[df['store_id'] == store_id]\n",
    "    df_short = df_short[df_short['date'] < date]\n",
    "    serie = df_short[variable]\n",
    "    \n",
    "    try:\n",
    "        b = np.mean(serie[-30:]) / np.mean(serie[:-30]) > 1\n",
    "        \n",
    "    except:\n",
    "        b = 0\n",
    "        \n",
    "    return b\n",
    "\n",
    "df['hausse_reduction'] = df.apply(lambda row : compute_hausse_reduction(row.date, row.store_id, 'reduction'), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression des variables non intéressantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['parent_chain_id', 'store_country', 'country_iso_code', 'region_id', 'store_activity_name', 'item_id', 'item_name', 'currency_code',\n",
    "                       'pickup_end', 'pickup_start', 'declared_supply', 'manual_removed_supply', 'store_cancellation', 'item_price',\n",
    "                       'meals_refunded', 'rating_count', 'sum_rating_overall', 'item_view', 'no_unique_consumers', 'is_enabled', 'Département', 'store_id', 'target'])\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((time.time() - t0)/ 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise en forme des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_var = ['store_region', 'store_segment']\n",
    "for var in categ_var:\n",
    "    df = pd.concat([df, pd.get_dummies(df[var], prefix = var)], axis = 1).drop(columns = [var])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lifetime'] = df['lifetime'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = min(df['items_first_enabled_date'])\n",
    "\n",
    "date_m = df['date'] - m\n",
    "items_first_enabled_date_m = df['items_first_enabled_date']-m\n",
    "store_first_saving_date_m = df['store_first_saving_date']-m\n",
    "store_last_saving_date_m = df['store_last_saving_date']-m\n",
    "\n",
    "df.drop(columns = ['date'])\n",
    "df.drop(columns = ['items_first_enabled_date'])\n",
    "df.drop(columns = ['store_first_saving_date'])\n",
    "df.drop(columns = ['store_last_saving_date'])\n",
    "\n",
    "df['date'] = date_m.apply(lambda x: x.days)\n",
    "df['items_first_enabled_date'] = items_first_enabled_date_m.apply(lambda x: x.days)\n",
    "df['store_first_saving_date'] = store_first_saving_date_m.apply(lambda x: x.days)\n",
    "df['store_last_saving_date'] = store_last_saving_date_m.apply(lambda x: x.days)\n",
    "\n",
    "\n",
    "df['temps_ouverture'] = df['temps_ouverture'].apply(lambda x: x.seconds/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['before_price', 'note_moyenne', 'reduction', 'efficacite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "df[list(df.columns)] = min_max_scaler.fit_transform(df[list(df.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application des modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit les échantillons\n",
    "\n",
    "y = df[['absence_future']]\n",
    "X = df.drop(columns = ['absence_future'])\n",
    "\n",
    "# On choisit un échantillon de validation de 20 %\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# On prédit à partir de l'échantillon de test pour calculer les scores\n",
    "\n",
    "y_pred = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(pd.DataFrame([\n",
    "    (X.columns[i], lin_reg.coef_[0][i]) for i in range(N)\n",
    "                    ]).T.rename(index = {0 : 'variable', 1 : 'coeff'}).T, x = 'variable', y = 'coeff', histfunc = 'sum'\n",
    "            ).show()\n",
    "\n",
    "print('MSE :', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des p-values\n",
    "\n",
    "mod = sm.OLS(y,X)\n",
    "fii = mod.fit()\n",
    "p_values = fii.summary2().tables[1]['P>|t|']\n",
    "pd.DataFrame(p_values).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_reg = ElasticNet(alpha=.1, copy_X=True, fit_intercept = False, l1_ratio=.031)\n",
    "\n",
    "EN_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = EN_reg.predict(X_test)\n",
    "print('MSE : ', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([(X.columns[i], EN_reg.coef_[i]) for i in range(N)]).T.rename(index = {0 : 'variable', 1 : 'coeff'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit d'abord une fonction qui donne l'importance des variables vis à vis d'un certain modèle\n",
    "\n",
    "def feat_importance(model, x_train, y_train, X):\n",
    "    \"\"\"\n",
    "    Renvoie le tableau de l'importance des variables vis à vis du modèle par la méthode des permutations\n",
    "    \"\"\"\n",
    "\n",
    "    result = permutation_importance(\n",
    "                                    model, \n",
    "                                    X, \n",
    "                                    y, \n",
    "                                    n_repeats = 3,\n",
    "                                    random_state = 0\n",
    "                                    )['importances_mean']\n",
    "    \n",
    "    importance = pd.DataFrame(result, index = X.columns, columns = [\"Importance\"])\n",
    "    \n",
    "    return importance.sort_values(by = ['Importance'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des essembles de tests et d'entrainement, on choisit une taile de test de 30% ici\n",
    "\n",
    "X = df.drop(['absence_future'], axis = 1)\n",
    "x = np.array(X)\n",
    "y = np.array(df['absence_future'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 41)\n",
    "# random_state correspond à la graine générant l'échantillon aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_ls = [1, 10, 13, 15, 17, 20, 25, 30] # profondeurs maximales des arbres de décision testées\n",
    "mse_train_max_depth = []\n",
    "mse_test_max_depth = []\n",
    "\n",
    "# Pour chaque profondeur max, on regresse avec random forest\n",
    "\n",
    "for m in max_depth_ls :\n",
    "    \n",
    "    print('Profondeur téstée : ', m)\n",
    "    \n",
    "    rf = RandomForestRegressor(\n",
    "                            max_depth = m, \n",
    "                            random_state=0,\n",
    "                            n_estimators = 30) # nombre d'arbres utilisés\n",
    "    \n",
    "    rf = rf.fit(x_train, y_train)\n",
    "    y_pred_train = rf.predict(x_train)\n",
    "    y_pred = rf.predict(x_test)\n",
    "    \n",
    "    mse_train_max_depth.append(mean_squared_error(y_train, y_pred_train))\n",
    "    mse_test_max_depth.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche ensuite les performances de la regression sur les deux échantillon (train et test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(max_depth_ls, mse_train_max_depth, color = 'red', label = 'Train')\n",
    "plt.plot(max_depth_ls, mse_test_max_depth, color = 'blue', label = 'Test')\n",
    "plt.title('MSE en fonction de max_depth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On regarde la valeur qui minimise la MSE sur l'ensemble de test\n",
    "\n",
    "max_depth_ls[mse_test_max_depth.index(min(mse_test_max_depth))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarde maintenant l'effet du nombre d'arbre sur l'effet de la regression\n",
    "\n",
    "nb_estimators_ls = [1, 2, 3, 5, 20, 40, 50, 60, 80]\n",
    "mse_train_nb_estimators = []\n",
    "mse_test_nb_estimators = []\n",
    "\n",
    "for m in nb_estimators_ls :\n",
    "    print(\"Nombre d'arbres testés : \", m)\n",
    "    rf = RandomForestRegressor(max_depth = 15, \n",
    "                               random_state = 0,\n",
    "                                n_estimators = m)    \n",
    "    \n",
    "    rf = rf.fit(x_train, y_train)\n",
    "    y_pred_train = rf.predict(x_train)\n",
    "    y_pred = rf.predict(x_test)\n",
    "    \n",
    "    mse_train_nb_estimators.append(mean_squared_error(y_train, y_pred_train))\n",
    "    mse_test_nb_estimators.append(mean_squared_error(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche ensuite les performances de la regression sur les deux échantillon (train et test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(nb_estimators_ls, mse_train_nb_estimators, color = 'red', label = 'Train')\n",
    "plt.plot(nb_estimators_ls, mse_test_nb_estimators, color = 'blue', label = 'Test')\n",
    "plt.title('MSE en fonction de n_estimators')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On fait varier le nombre minimum d'exemple requis pour créer une feuille/noeud\n",
    "\n",
    "samples_leaf_ls = [1, 2, 3, 4, 10]\n",
    "mse_train_samples_leaf = []\n",
    "mse_test_samples_leaf = []\n",
    "\n",
    "\n",
    "for m in samples_leaf_ls :\n",
    "    print('min_samples_leaf testé : ', m)\n",
    "    rf = RandomForestRegressor( max_depth = 15, \n",
    "                                min_samples_leaf = m,\n",
    "                                n_estimators = 60, \n",
    "                                random_state = 0\n",
    "                              )    \n",
    "    \n",
    "    rf = rf.fit(x_train, y_train)\n",
    "    y_pred_train = rf.predict(x_train)\n",
    "    y_pred = rf.predict(x_test)\n",
    "    \n",
    "    mse_train_samples_leaf.append(mean_squared_error(y_train, y_pred_train))\n",
    "    mse_test_samples_leaf.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche ensuite les performances de la regression sur les deux échantillon (train et test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(samples_leaf_ls, mse_train_samples_leaf, color='red', label='Train')\n",
    "plt.plot(samples_leaf_ls, mse_test_samples_leaf, color='blue', label='Test')\n",
    "plt.title('MSE en fct de min samples leaf')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_leaf_ls = [2, 10, 100, 150, 200, 1000, 1500]\n",
    "mse_train_max_leaf = []\n",
    "mse_test_max_leaf = []\n",
    "\n",
    "\n",
    "\n",
    "for m in max_leaf_ls :\n",
    "    \n",
    "    print('Nombre de feuilles max testé : ', m)\n",
    "    rf = RandomForestRegressor(max_depth = 15, \n",
    "                               min_samples_leaf = 1, \n",
    "                               max_leaf_nodes = m,\n",
    "                               n_estimators = 60)   \n",
    "    \n",
    "    rf = rf.fit(x_train, y_train)\n",
    "    y_pred_train = rf.predict(x_train)\n",
    "    y_pred = rf.predict(x_test)\n",
    "    mse_train_max_leaf.append(mean_squared_error(y_train, y_pred_train))\n",
    "    mse_test_max_leaf.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche ensuite les performances de la regression sur les deux échantillon (train et test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(max_leaf_ls, mse_train_max_leaf, color = 'red', label = 'Train')\n",
    "plt.plot(max_leaf_ls, mse_test_max_leaf, color = 'blue', label = 'Test')\n",
    "plt.title('MSE en fonction max_leaf_nodes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a maintenant tous nos paramètres\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "                        max_depth = 15, \n",
    "                        min_samples_leaf = 1, \n",
    "                        max_leaf_nodes = 1000,\n",
    "                        n_estimators = 60\n",
    "                            )    \n",
    "\n",
    "rf = rf.fit(x_train, y_train)\n",
    "y_pred_train = rf.predict(x_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "oo = np.zeros(y_pred.shape)\n",
    "\n",
    "print('MSE train : ', mean_squared_error(y_train, y_pred_train))\n",
    "print('MSE test : ', mean_squared_error(y_test, y_pred))\n",
    "print('MSE modèle nulle : ', mean_squared_error(y_test, oo))\n",
    "\n",
    "importance = feat_importance(rf, x_train, y_train, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance.plot(kind = 'barh', figsize = (18, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit les échantillons\n",
    "\n",
    "y = df[['absence_future']]\n",
    "X = df.drop(columns = ['absence_future'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction de l'architecture du réseau\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(70, input_dim = N, activation = 'relu'))\n",
    "\n",
    "model.add(Dense(12))\n",
    "model.add(Dense(13))\n",
    "model.add(Dense(15))\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancement de la phase d'apprentissage\n",
    "\n",
    "history = model.fit(X, y, validation_split = 0.2,  epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche l'évolution de la loss au fil des époques pour les échantillons train et test.\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Evolution de MSE sur X_train au fil des époques')\n",
    "plt.ylabel('mse')\n",
    "plt.legend(['train'], loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Evolution de MSE sur X_test au fil des époques')\n",
    "plt.ylabel('mse')\n",
    "plt.legend(['test'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(model, scoring = 'r2', random_state = 1).fit(X,y)\n",
    "eli5.show_weights(perm, feature_names = X.columns.tolist(), top = N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((time.time() - t0)/ 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
